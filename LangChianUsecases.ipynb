{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Use Cases\n",
    "\n",
    "From the tutorial [gkamradt/langchain-tutorials](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb)\n",
    "\n",
    "### Table of Contents\n",
    "- [Summarization](#Summarization)\n",
    "- [Q&A over documents](#qa-over-documents)\n",
    "- Extraction\n",
    "- Evaluation\n",
    "- Querying tabular data\n",
    "- Code understanding\n",
    "- Interacting with APIs\n",
    "- Chatbots\n",
    "- Agents\n",
    "\n",
    "\n",
    "There are also some wonderful projects & use cases: https://github.com/gkamradt/langchain-tutorials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "For now, I would use `OPEN_AI_API` temporarily when learning LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m load_dotenv()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set the OPENAI_API_KEY if not yet\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKeyIfNotSet')\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    " \n",
    "# Set the OPENAI_API_KEY if not yet\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKeyIfNotSet')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# print(openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarize from a bunch of information with the help from LLMs, like texts (possibly be books, articles, documents in any fields, table, financial reports, user manuals, podcasts, online posts, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Summaries of short text\n",
    "\n",
    "Just simply prompt with instructions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Init a model, specify model, pass in the keys\n",
    "# text-davinci-003 is alreay by dedault actually\n",
    "llm = OpenAI(\n",
    "    temperature=0, model_name=\"text-davinci-003\", openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Create the template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplte object for later use, utilizing the defined template and var text\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "confusing_text = \"\"\"\n",
    "Shortly after graduating from Cambridge, he got a job as a naturalist on the ship H.M.S. Beagle, which was about to start on a scientific and cartographic survey of the South American coast. The journey started in December 1831 and was to last almost five years, during which time he amassed considerable documentation.\n",
    "Darwin was particularly struck by the fauna of the south seas, notably by the tortoises he found on the Galapagos Islands, a group of Pacific Islands where nature seemed different from nature in other lands. The Galapagos tortoises, Darwin observed, differed from island to island, and this, he deduced, implied different forms of evolution, since the animals obviously came from the same origins. Darwin was also struck by the iguanas he found, and observed that those which lived in water had heads suitable for finding food among stones, whilst those that lived on land had a sharper profile, more fit for a herbivorous animal.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Prompt Shows =====\n",
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Respond in a manner that a 5 year old would understand.\n",
      "\n",
      "%TEXT:\n",
      "\n",
      "Shortly after graduating from Cambridge, he got a job as a naturalist on the ship H.M.S. Beagle, which was about to start on a scientific and cartographic survey of the South American coast. The journey started in December 1831 and was to last almost five years, during which time he amassed considerable documentation.\n",
      "Darwin was particularly struck by the fauna of the south seas, notably by the tortoises he found on the Galapagos Islands, a group of Pacific Islands where nature seemed different from nature in other lands. The Galapagos tortoises, Darwin observed, differed from island to island, and this, he deduced, implied different forms of evolution, since the animals obviously came from the same origins. Darwin was also struck by the iguanas he found, and observed that those which lived in water had heads suitable for finding food among stones, whilst those that lived on land had a sharper profile, more fit for a herbivorous animal.\n",
      "\n",
      "\n",
      "===== Prompt Ends =====\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"===== Prompt Shows =====\")\n",
    "\n",
    "final_prompt=prompt.format(text=confusing_text)\n",
    "print(final_prompt)\n",
    "\n",
    "print(\"===== Prompt Ends =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Darwin was a scientist who went on a long trip on a boat called the H.M.S. Beagle. He went to the Galapagos Islands and saw lots of animals like tortoises and iguanas. He noticed that the animals were different from island to island, which made him think that they had changed over time. He also noticed that the iguanas had different shaped heads depending on where they lived.\n"
     ]
    }
   ],
   "source": [
    "output=llm(final_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Summaries of long text\n",
    "\n",
    "\n",
    "For longer texts there might be socalled token limits constraining the number of texts you can pass in at one time. This way we can use the provided tools like text splitter and chain to solve the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Horgenschlag knew it, Doris Hillman and things were filing away Shirley Lester in the back of his mind. And Shirley Lester, the thought of her, no longer was available.  And that‚Äôs why I never wrote a boy-meets-girl story for Collier‚Äôs. In a boy-meets-girl story the boy should always meet the girl.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "with open('theheartofabrokenstory.txt','r') as file:\n",
    "    text=file.read()\n",
    "print(text[-300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4472 tokens in this file\n"
     ]
    }
   ],
   "source": [
    "num_token=llm.get_num_tokens(text)\n",
    "\n",
    "print(f'There are {num_token} tokens in this file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as seen, 4,472 tokens might be too long for one query. Now it would be splitted or called chunked into smaller parts. There are many methods can be utilized for this [can be read here](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html), but in this doc the `RecursiveCharacterTextSplitter()` would be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 4 docs for the original text file, e,g, the number of the tokens in the first doc of splitted text is 1027.\n"
     ]
    }
   ],
   "source": [
    "# [\"\\n\\n\", \"\\n\", \" \", \"\"] is the default list of separators, and proved necessary here\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=5000, chunk_overlap=350 )\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(f'Now we have {len(docs)} docs for the original text file, e,g, the number of the tokens in the first doc of splitted text is {llm.get_num_tokens(str(docs[0]))}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use a **chain** to combine them. Here we use chain type `map_reduce` thus we can get concise summaries. Type `map_reduce` is a chian specifically for summarizing. See how `map_reduce` type works [here.](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py)\n",
    "\n",
    "About different chain types and token limit workarounds, watch [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The Heart of a Broken Story Esquire XVI, September 1941  The only real difficulty in concocting a boy-meets-girl story is that, somehow, he must EVERY day Justin Horgenschlag, thirty-dollar-a-week printer‚Äôs assistant, saw at close quarters approximately sixty women whom he had never seen before. Thus in the few years he had lived in New York, Horgenschlag had seen at close quarters about 75,120 different women. Of these 75,120 women, roughly 25,000 were under thirty years of age and over fifteen years of age. Of the 25,000 only 5,000 weighed between one hundred five and one hundred twenty-five pounds. Of these 5,000 only 1,000 were not ugly. Only 500 were reasonably attractive; only 100 of these were quite attractive; only 25 could have inspired a long, slow whistle. And with only 1 did Horgenschlag fall in love at first sight.  Now, there are two kinds of femme fatale. There is the femme fatale who is a femme fatale in every sense of the word, and there is the femme fatale who is not a femme fatale in every sense of the word. Her name was Shirley Lester. She was twenty years old (eleven years younger than Horgenschlag), was five-foot-four (bringing her head to the level of Horgenschlag‚Äôs eyes), weighed 117 pounds (light as a feather to carry). Shirley was a stenographer, lived with and supported her mother, Agnes Lester, an old Nelson Eddy fan. In reference to Shirley‚Äôs looks people often put it this way: ‚ÄúShirley‚Äôs as pretty as a picture.‚Äù And in the Third Avenue bus early one morning, Horgenschlag stood over Shirley Lester, and was a dead duck. All because Shirley‚Äôs mouth was open in a peculiar way. Shirley was reading a cosmetic advertisement in the wall panel of the bus; and when Shirley read, Shirley relaxed slightly at the jaw. And in that short moment while Shirley‚Äôs mouth was open, lips were parted, Shirley was probably the most fatal one in all Manhattan. Horgenschlag saw in her a positive cure-all for a gigantic monster of loneliness which had been stalking around his heart since he had come to New York. Oh, the agony of it! The agony of standing over Shirley Lester and not being able to bend down and kiss Shirley‚Äôs parted lips. The inexpressible agony of it! ***  That was the beginning of the story I started to write for Collier‚Äôs. I was going to write a lovely tender boy-meets-girl story. What could be finer, I thought. The world needs boymeets-girl stories. But to write one, unfortunately, the writer must go about the business of having the boy meet the girl. I couldn‚Äôt do it with this one. Not and have it make sense. I couldn‚Äôt get Horgenschlag and Shirley together properly. And here are the reasons:  Certainly it was impossible for Horgenschlag to bend over and say in all sincerity: ‚ÄúI beg your pardon. I love you very much. I‚Äôm nuts about you. I know it. I could love you all my life. I‚Äôm a printer‚Äôs assistant and I make thirty dollars a week. Gosh, how I love you. Are you busy tonight?‚Äù  This Horgenschlag may be a goof, but not that big a goof. He may have been born yesterday, but not today. You can‚Äôt expect Collier‚Äôs readers to swallow that kind of bilge. A nickel‚Äôs a nickel, after all.  I couldn‚Äôt, of course, all of a sudden give Horgenschlag a suave serum, mixed from William Powell‚Äôs old cigarette case and Fred Astaire‚Äôs old top hat.  ‚ÄúPlease don‚Äôt misunderstand me, Miss. I‚Äôm a magazine illustrator. My card. I‚Äôd like to sketch you more than I‚Äôve ever wanted to sketch anyone in my life. Perhaps such an undertaking would be to a mutual advantage. May I telephone you this evening, or in Twenty-One Stories Train Wreck Recluse [ 13 ] the very near future? (Short, debonair laugh.) I hope I don‚Äôt sound too desperate. (Another one.) I suppose I am, really.‚Äù  Oh, boy. Those lines delivered with a weary, yet gay, yet reckless smile. If only Horgenschlag had delivered them. Shirley, of course, was an old Nelson Eddy fan herself, and an active member of the Keystone Circulating Library.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Maybe you‚Äôre beginning to see what I was up against.  True, Horgenschlag might have said the following:  ‚ÄúExcuse me, but aren‚Äôt you Wilma Pritchard?‚Äù  To which Shirley would have replied coldly, and seeking a neutral point on the other side of the bus:  ‚ÄúNo.‚Äù  ‚ÄúThat‚Äôs funny,‚Äù Horgenschlag could have gone on, ‚ÄúI was willing to swear you were Wilma Pritchard. Uh. You don‚Äôt by any chance come from Seattle?‚Äù  ‚ÄúNo.‚Äù‚ÄîMore ice where that came from.  ‚ÄúSeattle‚Äôs my home town.‚Äù Neutral point.  ‚ÄúGreat little town, Seattle. I mean it‚Äôs really a great little town. I‚Äôve only been here‚ÄîI mean in New York‚Äîfour years. I‚Äôm a printer‚Äôs assistant. Justin Horgenschlag is my name.‚Äù  ‚ÄúI‚Äôm really not inter-ested.‚Äù  Oh, Horgenschlag wouldn‚Äôt have got anywhere with that kind of line. He had neither the looks, personality, or good clothes to gain Shirley‚Äôs interest under the circumstances. He didn‚Äôt have a chance. And, as I said before, to write a really good boy-meets-girl story it‚Äôs wise to have the boy meet the girl.  Maybe Horgenschlag might have fainted, and in doing so grabbed for support: the support being Shirley‚Äôs ankle. He could have torn the stocking that way, or succeeded in ornamenting it with a fine long run. People would have made room for the stricken Horgenschlag, and he would have got to his feet, mumbling: ‚ÄúI‚Äôm all right, thanks,‚Äù then, ‚ÄúOh, say! I‚Äôm terribly sorry, Miss. I‚Äôve torn your stocking. You must let me pay for it. I‚Äôm short of cash right now, but just give me your address.‚Äù  Shirley wouldn‚Äôt have given him her address. She just would have become embarrassed and inarticulate. ‚ÄúIt‚Äôs all right,‚Äù she would have said, wishing Horgenschlag hadn‚Äôt been born. And besides, the whole idea is illogical. Horgenschlag, a Seattle boy, wouldn‚Äôt have dreamed of clutching at Shirley‚Äôs ankle. Not in the Third Avenue Bus.  But what is more logical is the possibility that Horgenschlag might have got desperate. There are still a few men who love desperately. Maybe Horgenschlag was one. He might have snatched Shirley‚Äôs handbag and run with it toward the rear exit door. Shirley would have screamed. Men would have heard her, and remembered the Alamo or something. Horgenschlag‚Äôs flight, let‚Äôs say, is now arrested. The bus is stopped. Patrolman Wilson, who hasn‚Äôt made a good arrest in a long time, reports on the scene. What‚Äôs going on here? Officer, this man tried to steal my purse.  Horgenschlag is hauled into court. Shirley, of course, must attend session. They both give their addresses; thereby Horgenschlag is informed of the location of Shirley‚Äôs divine abode.  Judge Perkins, who can‚Äôt even get a good, really good cup of coffee in his own house, sentences Horgenschlag to a year in jail. Shirley bites her lip, but Horgenschlag is marched away.  In prison, Horgenschlag writes the following letter to Shirley Lester:  ‚ÄúDear Miss Lester: ‚ÄúI did not really mean to steal your purse. I just took it because I love you. You see I only wanted to get to know you. Will you please write me a letter sometime when you get the time? It gets pretty lonely here and I love you very much and maybe even you would come to see me some time if you get the time.  Your friend, Justin Horgenschlag‚Äù  Shirley shows the letter to all her friends. They say, ‚ÄúAh, it‚Äôs cute, Shirley.‚Äù Shirley agrees that it‚Äôs kind of cute in a way. Maybe she‚Äôll answer it. ‚ÄúYes! Answer it. Give‚Äôm a break. What‚Äôve ya got t‚Äôlose?‚Äù So Shirley answers Horgenschlag‚Äôs letter.  ‚ÄúDear Mr. Horgenschlag: ‚ÄúI received your letter and really feel very sorry about what has happened. Unfortunately there is very little we can do about it at this time, but I do feel abominable concerning the turn of events. However, your sentence is a short one and soon you will be out. The best of luck to you.  Sincerely yours,  Shirley Lester‚Äù  ‚ÄúDear Miss Lester:  ‚ÄúYou will never know how cheered up you made me feel when I received your letter. You should not feel abominable at all. It was all my fault for being so crazy so don‚Äôt feel that way at all. We get movies here once a week and it really is not so bad. I am 31 years of age and come from Seattle. I have been in New York 4 years and think it is a great town only once in a while you get pretty lonesome. You are the prettiest girl I have ever seen even in Seattle. I wish you would come to see me some Saturday afternoon during visiting hours 2 to 4 and I will pay your train fare.  Your friend,  Justin Horgenschlag‚Äù  Shirley would have shown this letter, too, to all her friends. But she would not answer this one. Anyone could see that this Horgenschlag was a goof. And after all. She had answered the first letter. If she answered this silly letter the thing might drag on for months and everything. She did all she could do for the man. And what a name. Horgenschlag.  Meanwhile, in prison Horgenschlag is having a terrible time, even though they have movies once a week. His cell-mates are Snipe Morgan and Slicer Burke, two boys from the back room, who see in\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"If she answered this silly letter the thing might drag on for months and everything. She did all she could do for the man. And what a name. Horgenschlag.  Meanwhile, in prison Horgenschlag is having a terrible time, even though they have movies once a week. His cell-mates are Snipe Morgan and Slicer Burke, two boys from the back room, who see in Horgenschlag‚Äôs face a resemblance to a chap in Chicago who once ratted on them. They are convinced that Ratface Ferrero and Justin Horgenschlag are one and the same person.  ‚ÄúBut I‚Äôm not Ratface Ferrero,‚Äù Horgenschlag tells them.  ‚ÄúDon‚Äôt gimme that,‚Äù says Slicer, knocking Horgenschlag‚Äôs meager food rations to the floor.  ‚ÄúBash his head in,‚Äù says Snipe.  ‚ÄúI tell ya I‚Äôm just here because I stole a girl‚Äôs purse on the Third Avenue Bus,‚Äù pleads Horgenschlag. ‚ÄúOnly I didn‚Äôt really steal it. I fell in love with her, and it was the only way I could get to know her.‚Äù  ‚ÄúDon‚Äôt gimme that,‚Äù says Slicer.  ‚ÄúBash his head in,‚Äù says Snipe. Then there is the day when seventeen prisoners try to make an escape. During play period in the recreation yard, Slicer Burke lures the warden‚Äôs niece, eight-year-old Lisbeth Sue, into his clutches. He puts his eight-by-twelve hands around the child‚Äôs waist and holds her up for the warden to see.  ‚ÄúHey, warden!‚Äù yells Slicer. ‚ÄúOpen up them gates or it‚Äôs curtains for the kid!‚Äù  ‚ÄúI‚Äôm not afraid, Uncle Bert!‚Äù calls out Lisbeth Sue.  ‚ÄúPut down that child, Slicer!‚Äù commands the warden, with all the impotence at his command.  But Slicer knows he has the warden just where he wants him. Seventeen men and a small blonde child walk out the gates. Sixteen men and a small blonde child walk out safely. A guard in the high tower thinks he sees a wonderful opportunity to shoot Slicer in the head, and thereby destroy the unity of the escaping group. But he misses, and succeeds only in shooting the small man walking nervously behind Slicer, killing him instantly.  Guess who?  And, thus, my plan to write a boy-meets-girl story for Collier‚Äôs, a tender, memorable love story, is thwarted by the death of my hero. Now, Horgenschlag never would have been among those seventeen desperate men if only he had not been made desperate and panicky by Shirley‚Äôs failure to answer his second letter. But the fact remains that she did not answer his second letter. She never in a hundred years would have answered it. I can‚Äôt alter facts.  And what a shame. What a pity that Horgenschlag, in prison, was unable to write the following letter to Shirley Lester:  ‚ÄúDear Miss Lester: ‚ÄúI hope a few lines will not annoy or embarrass you. I‚Äôm writing, Miss Lester, because I‚Äôd like you to know that I am not a common thief. I stole your bag, I want you to know, because I fell in love with you the moment I saw you on the bus. I could think of no way to become acquainted with you except by acting rashly‚Äîfoolishly, to be accurate. But then, one is a fool when one is in love.  ‚ÄúI loved the way your lips were so slightly parted. You represented the answer to everything to me. I haven‚Äôt been unhappy since I came to New York four years ago, but neither have I been happy. Rather, I can best describe myself as having been one of the thousands of young men in New York who simply exist.  ‚ÄúI came to New York from Seattle. I was going to become rich and famous and well-dressed and suave. But in four years I‚Äôve learned that I am not going to become rich and famous and welldressed and suave. I‚Äôm a good printer‚Äôs assistant, but that‚Äôs all I am. One day the printer got sick, and I had to take his place. What a mess I made of things, Miss Lester. No one would take my orders. The typesetters just sort of giggled when I would tell them to get to work. And I don‚Äôt blame them. I‚Äôm a fool when I give orders. I suppose I‚Äôm just one of the millions who was never meant to give orders. But I don‚Äôt mind anymore. There‚Äôs a twenty-three-year-old kid my boss just hired. He‚Äôs only twenty-three, and I am thirty-one and have worked at the same place for four years. But I know that one day he will become head printer, and I will be his assistant. But I don‚Äôt mind knowing this anymore.  ‚ÄúLoving you is the important thing, Miss Lester. There are some people who think love is sex and marriage and six o‚Äôclock-kisses and children, and perhaps it is, Miss Lester. But do you know what I think? I think love is a touch and yet not a touch. ‚ÄúI suppose it‚Äôs important to a woman that other people think of her as the wife of a man who is either rich, handsome, witty or popular. I‚Äôm not even popular. I‚Äôm not even hated. I‚Äôm just‚ÄîI‚Äôm just‚ÄîJustin Horgenschlag. I never make people gay, sad, angry, or even disgusted. I think people regard me as a nice guy, but that‚Äôs all.  ‚ÄúWhen I was a child no one pointed me out as being cute or bright or good-looking. If they had to say something they said I had sturdy little legs.  ‚ÄúI don‚Äôt expect an answer to this letter, Miss Lester. I would like an answer more than anything else in the world, but\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"or even disgusted. I think people regard me as a nice guy, but that‚Äôs all.  ‚ÄúWhen I was a child no one pointed me out as being cute or bright or good-looking. If they had to say something they said I had sturdy little legs.  ‚ÄúI don‚Äôt expect an answer to this letter, Miss Lester. I would like an answer more than anything else in the world, but truthfully I don‚Äôt expect one. I merely wanted you to know the truth. If my love for you has only led me to a new and great sorrow, only I am to blame. ‚ÄúPerhaps one day you will understand and forgive your blundering admirer,  Justin Horgenschlag‚Äù  Such a letter would be no more unlikely than the following:  ‚ÄúDear Mr. Horgenschlag: ‚ÄúI got your letter and loved it. I feel guilty and miserable that events have taken the turn they have. If only you had spoken to me instead of taking my purse! But then, I suppose I should have turned the conversational chill on you.  ‚ÄúIt‚Äôs lunch hour at the office, and I‚Äôm alone here writing to you. I felt that I wanted to be alone today at lunch hour. I felt that if I had to go have lunch with the girls at the Automat and they jabbered through the meal as usual, I‚Äôd suddenly scream.  ‚ÄúI don‚Äôt care if you‚Äôre not a success, or that you‚Äôre not handsome, or rich, or famous or suave. Once upon a time I would have cared. When I was in high school I was always in love with the Joe Glamor boys. Donald Nicolson, the boy who walked in the rain and knew all Shakespeare‚Äôs sonnets backwards. Bob Lacey, the handsome gink who could shoot a basket from the middle of the floor, with the score tied and the chukker almost over. Harry Miller, who was so shy and had such nice, durable brown eyes.  ‚ÄúBut that crazy part of my life is over.  ‚ÄúThe people in your office who giggled when you gave them orders are on my black list. I hate them as I‚Äôve never hated anybody.  ‚ÄúYou saw me when I had all my make-up on. Without it, believe me, I‚Äôm no raving beauty. Please write me when you‚Äôre allowed to have visitors. I‚Äôd like you to take a second look at me. I‚Äôd like to be sure that you didn‚Äôt catch me at a phony best.  ‚ÄúOh, how I wish you‚Äôd told the judge why you stole my purse! We might be together and able to talk over all the many things I think we have in common. ‚ÄúPlease let me know when I may come to see you.  Yours sincerely,  Shirley Lester‚Äù  But Justin Horgenschlag never got to know Shirley Lester. She got off at Fifty-Sixth Street, and he got off at Thirty-Second Street. That night Shirley Lester went to the movies with Howard Lawrence with whom she was in love. Howard thought Shirley was a darn good sport, but that was as far as it went. And Justin Horgenschlag that night stayed home and listened to the Lux Toilet Soap radio play. He thought about Shirley all night, all the next day, and very often during that month. Then all of a sudden he was introduced to Doris Hillman who was beginning to be afraid she wasn‚Äôt going to get a husband. And then before Justin Horgenschlag knew it, Doris Hillman and things were filing away Shirley Lester in the back of his mind. And Shirley Lester, the thought of her, no longer was available.  And that‚Äôs why I never wrote a boy-meets-girl story for Collier‚Äôs. In a boy-meets-girl story the boy should always meet the girl.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Justin Horgenschlag, a thirty-dollar-a-week printer's assistant, falls in love with Shirley Lester, a stenographer, at first sight on a bus. He is unable to express his feelings to her, however, as he is not suave enough to pull off the kind of romantic lines that would be expected in a boy-meets-girl story. Shirley is an old Nelson Eddy fan, and Justin is unable to win her over with his awkwardness.\n",
      "\n",
      " Justin Horgenschlag, a Seattle boy, meets Shirley Lester on a bus and attempts to gain her interest, but fails. He then gets desperate and attempts to steal her purse, but is arrested and sentenced to a year in jail. While in prison, he writes her a letter expressing his love for her, but she does not respond.\n",
      "\n",
      " Justin Horgenschlag is in prison for stealing a girl's purse on the Third Avenue Bus, though he did it out of love. He is being threatened by his cellmates, Snipe Morgan and Slicer Burke, who think he is Ratface Ferrero. During an escape attempt, Horgenschlag is killed by a guard. The story is a tragedy, as Horgenschlag never got to send a letter to the girl he loved, Shirley Lester, explaining why he stole her purse.\n",
      "\n",
      " Justin Horgenschlag wrote a letter to Shirley Lester expressing his love for her, but never got to meet her. Shirley Lester went to the movies with Howard Lawrence instead, and Justin Horgenschlag ended up meeting Doris Hillman and filing away Shirley Lester in the back of his mind. This is why the author never wrote a boy-meets-girl story for Collier's.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Justin Horgenschlag falls in love with Shirley Lester at first sight, but is unable to express his feelings. He attempts to win her over, but fails and is sentenced to a year in prison. While in prison, he writes her a letter expressing his love, but she does not respond. He is killed during an escape attempt and never gets to send the letter. Shirley Lester goes to the movies with Howard Lawrence instead.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "ouput = chain.run(docs)\n",
    "print(ouput)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A over documents\n",
    "\n",
    "[Official documentation about this.](https://python.langchain.com/en/latest/use_cases/question_answering.html)\n",
    "\n",
    "The basic two components:\n",
    "1. Tell it the context around the question\n",
    "2. Tell it the question\n",
    "i.e., `\"llm(your context + your question) = your answer\"`\n",
    "\n",
    "Some good usages are chatting own documents, asking questions to academic papers, creating study guides, reference medical information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simple Q&A\n",
    "\n",
    "Simple as the explained two components above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to split text involving regular expressions, you should use the re.split() method from the re module in Python. This method provides more advanced splitting capabilities using regular expressions and allows you to split a string based on a complex pattern defined by a regular expression. You can also use capture groups in the regular expression to include the delimiter in the resulting list.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "context=\"\"\"\n",
    "split(): This method is available for strings in Python and is used to split a string into a list of substrings based on a delimiter. The delimiter is specified as an argument to the split() method. Some special features of split() include: It can split a string into substrings based on whitespace by default.\n",
    "You can specify a custom delimiter, such as a specific character or sequence of characters.\n",
    "You can limit the number of splits using the optional maxsplit parameter.\n",
    "\n",
    "splitlines(): This method is also available for strings in Python and is used to split a string into a list of lines. It recognizes different line-ending characters such as \\n, \\r, or \\r\\n. Some special features of splitlines() include: It splits the string into lines while preserving the line-ending characters.\n",
    "By default, it removes the line-ending characters from the resulting list of lines. You can preserve them by passing True as an argument.\n",
    "\n",
    "re.split(): This method is part of the re module in Python and provides more advanced splitting capabilities using regular expressions. Some special features of re.split() include: It allows you to split a string based on a complex pattern defined by a regular expression.\n",
    "You can use capture groups in the regular expression to include the delimiter in the resulting list.\n",
    "It provides more flexibility in handling various splitting scenarios.\"\"\"\n",
    "\n",
    "question = \"What should I use when I wanna split text involving regular expressions?\"\n",
    "\n",
    "output = llm(context + question)\n",
    "\n",
    "# Did strip the text to remove the leading and trailing whitespace\n",
    "print (output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it would be really trouble when it is required to even select the correct data into context. This is called \"[document retrieval](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\" and tightly related to AI Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using embeddings\n",
    "\n",
    "Basically means using embedding to find answers through a longer given text, that is, splitting text, embedding the chunks, putting them in a Db for querying. [Here is a tutorial for doing a book.](https://www.youtube.com/watch?v=h0DHDp1FbmQ).\n",
    "\n",
    "A key point is selecting relevant chunks by **pulling similar texts based on comparisons of vector embeddings**.\n",
    "\n",
    "*Now I am going to use self-hosted models.*\n",
    "\n",
    "**update: So far serious performance and accuracy issues with local model, more study work required to be done. But 0 issue with smooth experience using OpenAI API, strange, performance difference should not be this big**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-chat.bin\n",
      "mpt_model_load: loading model from '/Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-chat.bin' - please wait ...\n",
      "mpt_model_load: n_vocab        = 50432\n",
      "mpt_model_load: n_ctx          = 2048\n",
      "mpt_model_load: n_embd         = 4096\n",
      "mpt_model_load: n_head         = 32\n",
      "mpt_model_load: n_layer        = 32\n",
      "mpt_model_load: alibi_bias_max = 8.000000\n",
      "mpt_model_load: clip_qkv       = 0.000000\n",
      "mpt_model_load: ftype          = 2\n",
      "mpt_model_load: ggml ctx size = 5653.09 MB\n",
      "mpt_model_load: kv self size  = 1024.00 MB\n",
      "mpt_model_load: ........................ done\n",
      "mpt_model_load: model size =  4629.02 MB / num tensors = 194\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "# The vectorstore we'll be using\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# The easy document loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The embedding engine that will convert our text to vectors, doc: https://api.python.langchain.com/en/latest/modules/embeddings.html#langchain.embeddings.HuggingFaceEmbeddings.embed_documents\\\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Locaing text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For loading local models\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "model_path=\"/Users/hann/Projects/RES_LLM/private-llm/models/\"\n",
    "#  models are ggml-mpt-7b-chat.bin, ggml-gpt4all-l13b-snoozy.bin, ggml-gpt4all-j-v1.3-groovy.bin, ggml-mpt-7b-instruct.bin, ggml-v3-13b-hermes-q5_1.bin\n",
    "models={\n",
    "    \"groovy\": os.path.join(model_path, \"ggml-gpt4all-j-v1.3-groovy.bin\"),\n",
    "    \"snoozy\": os.path.join(model_path, \"ggml-gpt4all-l13b-snoozy.bin\"),\n",
    "    \"7bchat\": os.path.join(model_path, \"ggml-mpt-7b-chat.bin\"),\n",
    "    \"7binstruct\": os.path.join(model_path, \"ggml-mpt-7b-instruct.bin\"),\n",
    "    \"hermes\": os.path.join(model_path, \"ggml-v3-13b-hermes-q5_1.bin\")\n",
    "}\n",
    "\n",
    "#  \"groovy\", \"snoozy\",  \"7bchat\", \"7binstruct\", \"hermes\"\n",
    "md = \"7bchat\"\n",
    "llm = GPT4All(model=models[md], callbacks=[StreamingStdOutCallbackHandler()])\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 16523 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Document loading\n",
    "loader = TextLoader('theheartofabrokenstory.txt')\n",
    "doc = loader.load()\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 12 documents that have an average of 1,599 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding engine\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# print(openai_api_key)\n",
    "\n",
    "# embeddings_openai = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the retrieval engine and ask questions. The retriever will get similar documents and combine the question with the context. Then the LLM will answer the question.\n",
    "\n",
    "**The original chain has a bit problem (or outdated), now here is the workable version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=docsearch.as_retriever())\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The man described in these extracts is an office worker.  Justin's job title appears as \" head printer\" and we can infer from this that his profession or career must have been something related\n",
      " The man described in these extracts is an office worker.  Justin's job title appears as \" head printer\" and we can infer from this that his profession or career must have been something related"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The man described in these extracts is an office worker.  Justin\\'s job title appears as \" head printer\" and we can infer from this that his profession or career must have been something related'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"What does the author describe as good work?\"\n",
    "qa.run(\"What was the occupation of Justin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If you wanted to do more you would hook this up to a cloud vector database, use a tool like metal and start managing your documents, with external data sources\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "[Official documentation about this.](https://python.langchain.com/en/latest/use_cases/extraction.html)\n",
    "\n",
    "Extraction is the process of parsing data from a piece of text. This is commonly used with output parsing in order to structure our data.\n",
    "\n",
    "E.g., extract a structured row from a sentence to insert into a database, extract multiple rows from a long document to insert into a database, extracting parameters from a user query to make an API call. \n",
    "\n",
    "A popular library for extraction is [Kor](https://eyurtsev.github.io/kor/), check for advanced usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "\n",
    "# For loading local models\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "model_path=\"/Users/hann/Projects/RES_LLM/private-llm/models/\"\n",
    "#  models are ggml-mpt-7b-chat.bin, ggml-gpt4all-l13b-snoozy.bin, ggml-gpt4all-j-v1.3-groovy.bin, ggml-mpt-7b-instruct.bin, ggml-v3-13b-hermes-q5_1.bin\n",
    "models={\n",
    "    \"groovy\": os.path.join(model_path, \"ggml-gpt4all-j-v1.3-groovy.bin\"),\n",
    "    \"snoozy\": os.path.join(model_path, \"ggml-gpt4all-l13b-snoozy.bin\"),\n",
    "    \"7bchat\": os.path.join(model_path, \"ggml-mpt-7b-chat.bin\"),\n",
    "    \"7binstruct\": os.path.join(model_path, \"ggml-mpt-7b-instruct.bin\"),\n",
    "    \"hermes\": os.path.join(model_path, \"ggml-v3-13b-hermes-q5_1.bin\")\n",
    "}\n",
    "#  \"groovy\", \"snoozy\",  \"7bchat\", \"7binstruct\", \"hermes\"\n",
    "md = \"groovy\"\n",
    "\n",
    "# 1 for local, 0 for huggingface\n",
    "use_local = 1 \n",
    "\n",
    "# comment this line for using local model\n",
    "use_local = 0\n",
    "\n",
    "if use_local==0:\n",
    "    chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "else:\n",
    "    chat_model = GPT4All(model=models[md])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Vanilla extraction\n",
    "\n",
    "Simple example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"{'Apple': 'üçé', 'Pear': 'üçê', 'kiwi': 'ü•ù'}\" additional_kwargs={} example=False\n",
      "<class 'langchain.schema.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\"\n",
    "\n",
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = str(instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "# output = chat_model(prompt)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn this into a proper python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 'üçé', 'Pear': 'üçê', 'kiwi': 'ü•ù'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LangChain's Response Schema\n",
    "LangChain's response schema will does two things for us:\n",
    "\n",
    "Autogenerate a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
    "\n",
    "Read the output from the LLM and turn it into a proper python object for me\n",
    "\n",
    "Here I define the schema I want. I'm going to pull out the song and artist that a user wants to play from a pseudo chat message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# The schema I want it to output\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "print (fruit_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "fruit_output = chat_model(fruit_query.to_messages())\n",
    "output = output_parser.parse(fruit_output.content)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "[Official documentation about this.](https://python.langchain.com/en/latest/use_cases/evaluation.html)\n",
    "\n",
    "Evaluation is the process of doing quality checks on the output. Normally, deterministic code has tests we can run, but judging the output of LLMs is more difficult because of the unpredictableness and variability of natural language. LangChain provides tools that aid us in this journey.\n",
    "\n",
    "E.g., Run quality checks on your summarization or Question & Answer pipelines, check the output of you summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-instruct.bin\n",
      "mpt_model_load: loading model from '/Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-instruct.bin' - please wait ...\n",
      "mpt_model_load: n_vocab        = 50432\n",
      "mpt_model_load: n_ctx          = 2048\n",
      "mpt_model_load: n_embd         = 4096\n",
      "mpt_model_load: n_head         = 32\n",
      "mpt_model_load: n_layer        = 32\n",
      "mpt_model_load: alibi_bias_max = 8.000000\n",
      "mpt_model_load: clip_qkv       = 0.000000\n",
      "mpt_model_load: ftype          = 2\n",
      "mpt_model_load: ggml ctx size = 5653.09 MB\n",
      "mpt_model_load: kv self size  = 1024.00 MB\n",
      "mpt_model_load: ........................ done\n",
      "mpt_model_load: model size =  4629.02 MB / num tensors = 194\n"
     ]
    }
   ],
   "source": [
    "# Embeddings, store, and retrieval\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Model and doc loader\n",
    "# from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Eval!\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "# For loading local models\n",
    "from langchain.llms import GPT4All\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "model_path=\"/Users/hann/Projects/RES_LLM/private-llm/models/\"\n",
    "#  models are ggml-mpt-7b-chat.bin, ggml-gpt4all-l13b-snoozy.bin, ggml-gpt4all-j-v1.3-groovy.bin, ggml-mpt-7b-instruct.bin, ggml-v3-13b-hermes-q5_1.bin\n",
    "models={\n",
    "    \"groovy\": os.path.join(model_path, \"ggml-gpt4all-j-v1.3-groovy.bin\"),\n",
    "    \"snoozy\": os.path.join(model_path, \"ggml-gpt4all-l13b-snoozy.bin\"),\n",
    "    \"7bchat\": os.path.join(model_path, \"ggml-mpt-7b-chat.bin\"),\n",
    "    \"7binstruct\": os.path.join(model_path, \"ggml-mpt-7b-instruct.bin\"),\n",
    "    \"hermes\": os.path.join(model_path, \"ggml-v3-13b-hermes-q5_1.bin\")\n",
    "}\n",
    "#  \"groovy\", \"snoozy\",  \"7bchat\", \"7binstruct\", \"hermes\"\n",
    "md = \"7binstruct\"\n",
    "\n",
    "# 1 for local, 0 for huggingface\n",
    "use_local = 1 \n",
    "\n",
    "# comment this line for using local model\n",
    "# use_local = 0\n",
    "\n",
    "if use_local==0:\n",
    "    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "else:\n",
    "    llm = GPT4All(model=models[md])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 16523 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Our long essay from before\n",
    "loader = TextLoader('theheartofabrokenstory.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 8 documents that have an average of 2,310 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding engine\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Embeddings and docstore\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your retrieval chain. Notice how I have an input_key parameter now. This tells the chain which key from a dictionary I supply has my prompt/query in it. I specify question to match the question in the dict below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=docsearch.as_retriever(), input_key=\"question\")\n",
    "\n",
    "# qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "# chain = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever(),input_key=\"question\")\n",
    "# /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll pass a list of questions and ground truth answers to the LLM that I know are correct (I validated them as a human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"What is the occupation of Justin?\", 'answer' : 'printer‚Äôs assistant'},\n",
    "    {'question' : \"In which city the story has happened\", 'answer' : 'New York'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposed to be like:\n",
    "\n",
    "```json\n",
    "[{'question': 'What is the occupation of Justin?',\n",
    "  'answer': 'printer‚Äôs assistant',\n",
    "  'result': ' Jsutin is a printer‚Äôs assistant.'},\n",
    " {'question': 'In which city the story has happened?',\n",
    "  'answer': 'New York',\n",
    "  'result': ' The story has happened in New York.'},]\n",
    "```\n",
    "\n",
    "I'll use chain.apply to run both my questions one by one separately.\n",
    "\n",
    "One of the cool parts is that I'll get my list of question and answers dictionaries back, but there'll be another key in the dictionary result which will be the output from the LLM.\n",
    "\n",
    "Note: I specifically made my 2nd question ambigious and tough to answer in one pass so the LLM would get it incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"I suppose I'm just one Of The Millions Who Was Never Meanth To Give Orders.\"\n",
      " \"I suppose me as having been one Of The Millions Who Was Never Meanth To Give Orders.\"\n",
      " \"I suppose I'm just one Of The Millions Who Was Never Meanth To Give Orders.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPT: reached the end of the context window so resizing\n",
      "Exception ignored on calling ctypes callback function: <function LLModel._recalculate_callback at 0x113c49e50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hann/miniconda3/envs/llm/lib/python3.8/site-packages/gpt4all/pyllmodel.py\", line 334, in _recalculate_callback\n",
      "    @staticmethod\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "predictions = chain.apply(question_answers)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have the LLM compare my ground truth answer (the answer key) with the result from the LLM (result key).\n",
    "\n",
    "Or simply, we are asking the LLM to grade itself. What a wild world we live in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INCORRECT\n",
      " INCORRECT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': ' INCORRECT'}, {'text': ' INCORRECT'}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     predictions,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')\n",
    "graded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
