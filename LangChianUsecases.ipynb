{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Use Cases\n",
    "\n",
    "From the tutorial [gkamradt/langchain-tutorials](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb)\n",
    "\n",
    "### Table of Contents\n",
    "- [Summarization](#Summarization)\n",
    "- [Q&A over documents](#qa-over-documents)\n",
    "- Extraction\n",
    "- Evaluation\n",
    "- Querying tabular data\n",
    "- Code understanding\n",
    "- Interacting with APIs\n",
    "- Chatbots\n",
    "- Agents\n",
    "\n",
    "\n",
    "There are also some wonderful projects & use cases: https://github.com/gkamradt/langchain-tutorials\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "For now, I would use `OPEN_AI_API` temporarily when learning LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      4\u001b[0m load_dotenv()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set the OPENAI_API_KEY if not yet\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKeyIfNotSet')\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    " \n",
    "# Set the OPENAI_API_KEY if not yet\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY', 'YourAPIKeyIfNotSet')\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# print(openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarize from a bunch of information with the help from LLMs, like texts (possibly be books, articles, documents in any fields, table, financial reports, user manuals, podcasts, online posts, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Summaries of short text\n",
    "\n",
    "Just simply prompt with instructions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Init a model, specify model, pass in the keys\n",
    "# text-davinci-003 is alreay by dedault actually\n",
    "llm = OpenAI(\n",
    "    temperature=0, model_name=\"text-davinci-003\", openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Create the template\n",
    "template = \"\"\"\n",
    "%INSTRUCTIONS:\n",
    "Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "%TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplte object for later use, utilizing the defined template and var text\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "confusing_text = \"\"\"\n",
    "Shortly after graduating from Cambridge, he got a job as a naturalist on the ship H.M.S. Beagle, which was about to start on a scientific and cartographic survey of the South American coast. The journey started in December 1831 and was to last almost five years, during which time he amassed considerable documentation.\n",
    "Darwin was particularly struck by the fauna of the south seas, notably by the tortoises he found on the Galapagos Islands, a group of Pacific Islands where nature seemed different from nature in other lands. The Galapagos tortoises, Darwin observed, differed from island to island, and this, he deduced, implied different forms of evolution, since the animals obviously came from the same origins. Darwin was also struck by the iguanas he found, and observed that those which lived in water had heads suitable for finding food among stones, whilst those that lived on land had a sharper profile, more fit for a herbivorous animal.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Prompt Shows =====\n",
      "\n",
      "%INSTRUCTIONS:\n",
      "Please summarize the following piece of text.\n",
      "Respond in a manner that a 5 year old would understand.\n",
      "\n",
      "%TEXT:\n",
      "\n",
      "Shortly after graduating from Cambridge, he got a job as a naturalist on the ship H.M.S. Beagle, which was about to start on a scientific and cartographic survey of the South American coast. The journey started in December 1831 and was to last almost five years, during which time he amassed considerable documentation.\n",
      "Darwin was particularly struck by the fauna of the south seas, notably by the tortoises he found on the Galapagos Islands, a group of Pacific Islands where nature seemed different from nature in other lands. The Galapagos tortoises, Darwin observed, differed from island to island, and this, he deduced, implied different forms of evolution, since the animals obviously came from the same origins. Darwin was also struck by the iguanas he found, and observed that those which lived in water had heads suitable for finding food among stones, whilst those that lived on land had a sharper profile, more fit for a herbivorous animal.\n",
      "\n",
      "\n",
      "===== Prompt Ends =====\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"===== Prompt Shows =====\")\n",
    "\n",
    "final_prompt=prompt.format(text=confusing_text)\n",
    "print(final_prompt)\n",
    "\n",
    "print(\"===== Prompt Ends =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Darwin was a scientist who went on a long trip on a boat called the H.M.S. Beagle. He went to the Galapagos Islands and saw lots of animals like tortoises and iguanas. He noticed that the animals were different from island to island, which made him think that they had changed over time. He also noticed that the iguanas had different shaped heads depending on where they lived.\n"
     ]
    }
   ],
   "source": [
    "output=llm(final_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Summaries of long text\n",
    "\n",
    "\n",
    "For longer texts there might be socalled token limits constraining the number of texts you can pass in at one time. This way we can use the provided tools like text splitter and chain to solve the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Horgenschlag knew it, Doris Hillman and things were filing away Shirley Lester in the back of his mind. And Shirley Lester, the thought of her, no longer was available.  And that’s why I never wrote a boy-meets-girl story for Collier’s. In a boy-meets-girl story the boy should always meet the girl.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "with open('theheartofabrokenstory.txt','r') as file:\n",
    "    text=file.read()\n",
    "print(text[-300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4472 tokens in this file\n"
     ]
    }
   ],
   "source": [
    "num_token=llm.get_num_tokens(text)\n",
    "\n",
    "print(f'There are {num_token} tokens in this file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as seen, 4,472 tokens might be too long for one query. Now it would be splitted or called chunked into smaller parts. There are many methods can be utilized for this [can be read here](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html), but in this doc the `RecursiveCharacterTextSplitter()` would be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 4 docs for the original text file, e,g, the number of the tokens in the first doc of splitted text is 1027.\n"
     ]
    }
   ],
   "source": [
    "# [\"\\n\\n\", \"\\n\", \" \", \"\"] is the default list of separators, and proved necessary here\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \"\"], chunk_size=5000, chunk_overlap=350 )\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "print(f'Now we have {len(docs)} docs for the original text file, e,g, the number of the tokens in the first doc of splitted text is {llm.get_num_tokens(str(docs[0]))}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use a **chain** to combine them. Here we use chain type `map_reduce` thus we can get concise summaries. Type `map_reduce` is a chian specifically for summarizing. See how `map_reduce` type works [here.](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py)\n",
    "\n",
    "About different chain types and token limit workarounds, watch [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The Heart of a Broken Story Esquire XVI, September 1941  The only real difficulty in concocting a boy-meets-girl story is that, somehow, he must EVERY day Justin Horgenschlag, thirty-dollar-a-week printer’s assistant, saw at close quarters approximately sixty women whom he had never seen before. Thus in the few years he had lived in New York, Horgenschlag had seen at close quarters about 75,120 different women. Of these 75,120 women, roughly 25,000 were under thirty years of age and over fifteen years of age. Of the 25,000 only 5,000 weighed between one hundred five and one hundred twenty-five pounds. Of these 5,000 only 1,000 were not ugly. Only 500 were reasonably attractive; only 100 of these were quite attractive; only 25 could have inspired a long, slow whistle. And with only 1 did Horgenschlag fall in love at first sight.  Now, there are two kinds of femme fatale. There is the femme fatale who is a femme fatale in every sense of the word, and there is the femme fatale who is not a femme fatale in every sense of the word. Her name was Shirley Lester. She was twenty years old (eleven years younger than Horgenschlag), was five-foot-four (bringing her head to the level of Horgenschlag’s eyes), weighed 117 pounds (light as a feather to carry). Shirley was a stenographer, lived with and supported her mother, Agnes Lester, an old Nelson Eddy fan. In reference to Shirley’s looks people often put it this way: “Shirley’s as pretty as a picture.” And in the Third Avenue bus early one morning, Horgenschlag stood over Shirley Lester, and was a dead duck. All because Shirley’s mouth was open in a peculiar way. Shirley was reading a cosmetic advertisement in the wall panel of the bus; and when Shirley read, Shirley relaxed slightly at the jaw. And in that short moment while Shirley’s mouth was open, lips were parted, Shirley was probably the most fatal one in all Manhattan. Horgenschlag saw in her a positive cure-all for a gigantic monster of loneliness which had been stalking around his heart since he had come to New York. Oh, the agony of it! The agony of standing over Shirley Lester and not being able to bend down and kiss Shirley’s parted lips. The inexpressible agony of it! ***  That was the beginning of the story I started to write for Collier’s. I was going to write a lovely tender boy-meets-girl story. What could be finer, I thought. The world needs boymeets-girl stories. But to write one, unfortunately, the writer must go about the business of having the boy meet the girl. I couldn’t do it with this one. Not and have it make sense. I couldn’t get Horgenschlag and Shirley together properly. And here are the reasons:  Certainly it was impossible for Horgenschlag to bend over and say in all sincerity: “I beg your pardon. I love you very much. I’m nuts about you. I know it. I could love you all my life. I’m a printer’s assistant and I make thirty dollars a week. Gosh, how I love you. Are you busy tonight?”  This Horgenschlag may be a goof, but not that big a goof. He may have been born yesterday, but not today. You can’t expect Collier’s readers to swallow that kind of bilge. A nickel’s a nickel, after all.  I couldn’t, of course, all of a sudden give Horgenschlag a suave serum, mixed from William Powell’s old cigarette case and Fred Astaire’s old top hat.  “Please don’t misunderstand me, Miss. I’m a magazine illustrator. My card. I’d like to sketch you more than I’ve ever wanted to sketch anyone in my life. Perhaps such an undertaking would be to a mutual advantage. May I telephone you this evening, or in Twenty-One Stories Train Wreck Recluse [ 13 ] the very near future? (Short, debonair laugh.) I hope I don’t sound too desperate. (Another one.) I suppose I am, really.”  Oh, boy. Those lines delivered with a weary, yet gay, yet reckless smile. If only Horgenschlag had delivered them. Shirley, of course, was an old Nelson Eddy fan herself, and an active member of the Keystone Circulating Library.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Maybe you’re beginning to see what I was up against.  True, Horgenschlag might have said the following:  “Excuse me, but aren’t you Wilma Pritchard?”  To which Shirley would have replied coldly, and seeking a neutral point on the other side of the bus:  “No.”  “That’s funny,” Horgenschlag could have gone on, “I was willing to swear you were Wilma Pritchard. Uh. You don’t by any chance come from Seattle?”  “No.”—More ice where that came from.  “Seattle’s my home town.” Neutral point.  “Great little town, Seattle. I mean it’s really a great little town. I’ve only been here—I mean in New York—four years. I’m a printer’s assistant. Justin Horgenschlag is my name.”  “I’m really not inter-ested.”  Oh, Horgenschlag wouldn’t have got anywhere with that kind of line. He had neither the looks, personality, or good clothes to gain Shirley’s interest under the circumstances. He didn’t have a chance. And, as I said before, to write a really good boy-meets-girl story it’s wise to have the boy meet the girl.  Maybe Horgenschlag might have fainted, and in doing so grabbed for support: the support being Shirley’s ankle. He could have torn the stocking that way, or succeeded in ornamenting it with a fine long run. People would have made room for the stricken Horgenschlag, and he would have got to his feet, mumbling: “I’m all right, thanks,” then, “Oh, say! I’m terribly sorry, Miss. I’ve torn your stocking. You must let me pay for it. I’m short of cash right now, but just give me your address.”  Shirley wouldn’t have given him her address. She just would have become embarrassed and inarticulate. “It’s all right,” she would have said, wishing Horgenschlag hadn’t been born. And besides, the whole idea is illogical. Horgenschlag, a Seattle boy, wouldn’t have dreamed of clutching at Shirley’s ankle. Not in the Third Avenue Bus.  But what is more logical is the possibility that Horgenschlag might have got desperate. There are still a few men who love desperately. Maybe Horgenschlag was one. He might have snatched Shirley’s handbag and run with it toward the rear exit door. Shirley would have screamed. Men would have heard her, and remembered the Alamo or something. Horgenschlag’s flight, let’s say, is now arrested. The bus is stopped. Patrolman Wilson, who hasn’t made a good arrest in a long time, reports on the scene. What’s going on here? Officer, this man tried to steal my purse.  Horgenschlag is hauled into court. Shirley, of course, must attend session. They both give their addresses; thereby Horgenschlag is informed of the location of Shirley’s divine abode.  Judge Perkins, who can’t even get a good, really good cup of coffee in his own house, sentences Horgenschlag to a year in jail. Shirley bites her lip, but Horgenschlag is marched away.  In prison, Horgenschlag writes the following letter to Shirley Lester:  “Dear Miss Lester: “I did not really mean to steal your purse. I just took it because I love you. You see I only wanted to get to know you. Will you please write me a letter sometime when you get the time? It gets pretty lonely here and I love you very much and maybe even you would come to see me some time if you get the time.  Your friend, Justin Horgenschlag”  Shirley shows the letter to all her friends. They say, “Ah, it’s cute, Shirley.” Shirley agrees that it’s kind of cute in a way. Maybe she’ll answer it. “Yes! Answer it. Give’m a break. What’ve ya got t’lose?” So Shirley answers Horgenschlag’s letter.  “Dear Mr. Horgenschlag: “I received your letter and really feel very sorry about what has happened. Unfortunately there is very little we can do about it at this time, but I do feel abominable concerning the turn of events. However, your sentence is a short one and soon you will be out. The best of luck to you.  Sincerely yours,  Shirley Lester”  “Dear Miss Lester:  “You will never know how cheered up you made me feel when I received your letter. You should not feel abominable at all. It was all my fault for being so crazy so don’t feel that way at all. We get movies here once a week and it really is not so bad. I am 31 years of age and come from Seattle. I have been in New York 4 years and think it is a great town only once in a while you get pretty lonesome. You are the prettiest girl I have ever seen even in Seattle. I wish you would come to see me some Saturday afternoon during visiting hours 2 to 4 and I will pay your train fare.  Your friend,  Justin Horgenschlag”  Shirley would have shown this letter, too, to all her friends. But she would not answer this one. Anyone could see that this Horgenschlag was a goof. And after all. She had answered the first letter. If she answered this silly letter the thing might drag on for months and everything. She did all she could do for the man. And what a name. Horgenschlag.  Meanwhile, in prison Horgenschlag is having a terrible time, even though they have movies once a week. His cell-mates are Snipe Morgan and Slicer Burke, two boys from the back room, who see in\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"If she answered this silly letter the thing might drag on for months and everything. She did all she could do for the man. And what a name. Horgenschlag.  Meanwhile, in prison Horgenschlag is having a terrible time, even though they have movies once a week. His cell-mates are Snipe Morgan and Slicer Burke, two boys from the back room, who see in Horgenschlag’s face a resemblance to a chap in Chicago who once ratted on them. They are convinced that Ratface Ferrero and Justin Horgenschlag are one and the same person.  “But I’m not Ratface Ferrero,” Horgenschlag tells them.  “Don’t gimme that,” says Slicer, knocking Horgenschlag’s meager food rations to the floor.  “Bash his head in,” says Snipe.  “I tell ya I’m just here because I stole a girl’s purse on the Third Avenue Bus,” pleads Horgenschlag. “Only I didn’t really steal it. I fell in love with her, and it was the only way I could get to know her.”  “Don’t gimme that,” says Slicer.  “Bash his head in,” says Snipe. Then there is the day when seventeen prisoners try to make an escape. During play period in the recreation yard, Slicer Burke lures the warden’s niece, eight-year-old Lisbeth Sue, into his clutches. He puts his eight-by-twelve hands around the child’s waist and holds her up for the warden to see.  “Hey, warden!” yells Slicer. “Open up them gates or it’s curtains for the kid!”  “I’m not afraid, Uncle Bert!” calls out Lisbeth Sue.  “Put down that child, Slicer!” commands the warden, with all the impotence at his command.  But Slicer knows he has the warden just where he wants him. Seventeen men and a small blonde child walk out the gates. Sixteen men and a small blonde child walk out safely. A guard in the high tower thinks he sees a wonderful opportunity to shoot Slicer in the head, and thereby destroy the unity of the escaping group. But he misses, and succeeds only in shooting the small man walking nervously behind Slicer, killing him instantly.  Guess who?  And, thus, my plan to write a boy-meets-girl story for Collier’s, a tender, memorable love story, is thwarted by the death of my hero. Now, Horgenschlag never would have been among those seventeen desperate men if only he had not been made desperate and panicky by Shirley’s failure to answer his second letter. But the fact remains that she did not answer his second letter. She never in a hundred years would have answered it. I can’t alter facts.  And what a shame. What a pity that Horgenschlag, in prison, was unable to write the following letter to Shirley Lester:  “Dear Miss Lester: “I hope a few lines will not annoy or embarrass you. I’m writing, Miss Lester, because I’d like you to know that I am not a common thief. I stole your bag, I want you to know, because I fell in love with you the moment I saw you on the bus. I could think of no way to become acquainted with you except by acting rashly—foolishly, to be accurate. But then, one is a fool when one is in love.  “I loved the way your lips were so slightly parted. You represented the answer to everything to me. I haven’t been unhappy since I came to New York four years ago, but neither have I been happy. Rather, I can best describe myself as having been one of the thousands of young men in New York who simply exist.  “I came to New York from Seattle. I was going to become rich and famous and well-dressed and suave. But in four years I’ve learned that I am not going to become rich and famous and welldressed and suave. I’m a good printer’s assistant, but that’s all I am. One day the printer got sick, and I had to take his place. What a mess I made of things, Miss Lester. No one would take my orders. The typesetters just sort of giggled when I would tell them to get to work. And I don’t blame them. I’m a fool when I give orders. I suppose I’m just one of the millions who was never meant to give orders. But I don’t mind anymore. There’s a twenty-three-year-old kid my boss just hired. He’s only twenty-three, and I am thirty-one and have worked at the same place for four years. But I know that one day he will become head printer, and I will be his assistant. But I don’t mind knowing this anymore.  “Loving you is the important thing, Miss Lester. There are some people who think love is sex and marriage and six o’clock-kisses and children, and perhaps it is, Miss Lester. But do you know what I think? I think love is a touch and yet not a touch. “I suppose it’s important to a woman that other people think of her as the wife of a man who is either rich, handsome, witty or popular. I’m not even popular. I’m not even hated. I’m just—I’m just—Justin Horgenschlag. I never make people gay, sad, angry, or even disgusted. I think people regard me as a nice guy, but that’s all.  “When I was a child no one pointed me out as being cute or bright or good-looking. If they had to say something they said I had sturdy little legs.  “I don’t expect an answer to this letter, Miss Lester. I would like an answer more than anything else in the world, but\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"or even disgusted. I think people regard me as a nice guy, but that’s all.  “When I was a child no one pointed me out as being cute or bright or good-looking. If they had to say something they said I had sturdy little legs.  “I don’t expect an answer to this letter, Miss Lester. I would like an answer more than anything else in the world, but truthfully I don’t expect one. I merely wanted you to know the truth. If my love for you has only led me to a new and great sorrow, only I am to blame. “Perhaps one day you will understand and forgive your blundering admirer,  Justin Horgenschlag”  Such a letter would be no more unlikely than the following:  “Dear Mr. Horgenschlag: “I got your letter and loved it. I feel guilty and miserable that events have taken the turn they have. If only you had spoken to me instead of taking my purse! But then, I suppose I should have turned the conversational chill on you.  “It’s lunch hour at the office, and I’m alone here writing to you. I felt that I wanted to be alone today at lunch hour. I felt that if I had to go have lunch with the girls at the Automat and they jabbered through the meal as usual, I’d suddenly scream.  “I don’t care if you’re not a success, or that you’re not handsome, or rich, or famous or suave. Once upon a time I would have cared. When I was in high school I was always in love with the Joe Glamor boys. Donald Nicolson, the boy who walked in the rain and knew all Shakespeare’s sonnets backwards. Bob Lacey, the handsome gink who could shoot a basket from the middle of the floor, with the score tied and the chukker almost over. Harry Miller, who was so shy and had such nice, durable brown eyes.  “But that crazy part of my life is over.  “The people in your office who giggled when you gave them orders are on my black list. I hate them as I’ve never hated anybody.  “You saw me when I had all my make-up on. Without it, believe me, I’m no raving beauty. Please write me when you’re allowed to have visitors. I’d like you to take a second look at me. I’d like to be sure that you didn’t catch me at a phony best.  “Oh, how I wish you’d told the judge why you stole my purse! We might be together and able to talk over all the many things I think we have in common. “Please let me know when I may come to see you.  Yours sincerely,  Shirley Lester”  But Justin Horgenschlag never got to know Shirley Lester. She got off at Fifty-Sixth Street, and he got off at Thirty-Second Street. That night Shirley Lester went to the movies with Howard Lawrence with whom she was in love. Howard thought Shirley was a darn good sport, but that was as far as it went. And Justin Horgenschlag that night stayed home and listened to the Lux Toilet Soap radio play. He thought about Shirley all night, all the next day, and very often during that month. Then all of a sudden he was introduced to Doris Hillman who was beginning to be afraid she wasn’t going to get a husband. And then before Justin Horgenschlag knew it, Doris Hillman and things were filing away Shirley Lester in the back of his mind. And Shirley Lester, the thought of her, no longer was available.  And that’s why I never wrote a boy-meets-girl story for Collier’s. In a boy-meets-girl story the boy should always meet the girl.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Justin Horgenschlag, a thirty-dollar-a-week printer's assistant, falls in love with Shirley Lester, a stenographer, at first sight on a bus. He is unable to express his feelings to her, however, as he is not suave enough to pull off the kind of romantic lines that would be expected in a boy-meets-girl story. Shirley is an old Nelson Eddy fan, and Justin is unable to win her over with his awkwardness.\n",
      "\n",
      " Justin Horgenschlag, a Seattle boy, meets Shirley Lester on a bus and attempts to gain her interest, but fails. He then gets desperate and attempts to steal her purse, but is arrested and sentenced to a year in jail. While in prison, he writes her a letter expressing his love for her, but she does not respond.\n",
      "\n",
      " Justin Horgenschlag is in prison for stealing a girl's purse on the Third Avenue Bus, though he did it out of love. He is being threatened by his cellmates, Snipe Morgan and Slicer Burke, who think he is Ratface Ferrero. During an escape attempt, Horgenschlag is killed by a guard. The story is a tragedy, as Horgenschlag never got to send a letter to the girl he loved, Shirley Lester, explaining why he stole her purse.\n",
      "\n",
      " Justin Horgenschlag wrote a letter to Shirley Lester expressing his love for her, but never got to meet her. Shirley Lester went to the movies with Howard Lawrence instead, and Justin Horgenschlag ended up meeting Doris Hillman and filing away Shirley Lester in the back of his mind. This is why the author never wrote a boy-meets-girl story for Collier's.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Justin Horgenschlag falls in love with Shirley Lester at first sight, but is unable to express his feelings. He attempts to win her over, but fails and is sentenced to a year in prison. While in prison, he writes her a letter expressing his love, but she does not respond. He is killed during an escape attempt and never gets to send the letter. Shirley Lester goes to the movies with Howard Lawrence instead.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "ouput = chain.run(docs)\n",
    "print(ouput)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A over documents\n",
    "\n",
    "[Official documentation about this.](https://python.langchain.com/en/latest/use_cases/question_answering.html)\n",
    "\n",
    "The basic two components:\n",
    "1. Tell it the context around the question\n",
    "2. Tell it the question\n",
    "i.e., `\"llm(your context + your question) = your answer\"`\n",
    "\n",
    "Some good usages are chatting own documents, asking questions to academic papers, creating study guides, reference medical information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simple Q&A\n",
    "\n",
    "Simple as the explained two components above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to split text involving regular expressions, you should use the re.split() method from the re module in Python. This method provides more advanced splitting capabilities using regular expressions and allows you to split a string based on a complex pattern defined by a regular expression. You can also use capture groups in the regular expression to include the delimiter in the resulting list.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "context=\"\"\"\n",
    "split(): This method is available for strings in Python and is used to split a string into a list of substrings based on a delimiter. The delimiter is specified as an argument to the split() method. Some special features of split() include: It can split a string into substrings based on whitespace by default.\n",
    "You can specify a custom delimiter, such as a specific character or sequence of characters.\n",
    "You can limit the number of splits using the optional maxsplit parameter.\n",
    "\n",
    "splitlines(): This method is also available for strings in Python and is used to split a string into a list of lines. It recognizes different line-ending characters such as \\n, \\r, or \\r\\n. Some special features of splitlines() include: It splits the string into lines while preserving the line-ending characters.\n",
    "By default, it removes the line-ending characters from the resulting list of lines. You can preserve them by passing True as an argument.\n",
    "\n",
    "re.split(): This method is part of the re module in Python and provides more advanced splitting capabilities using regular expressions. Some special features of re.split() include: It allows you to split a string based on a complex pattern defined by a regular expression.\n",
    "You can use capture groups in the regular expression to include the delimiter in the resulting list.\n",
    "It provides more flexibility in handling various splitting scenarios.\"\"\"\n",
    "\n",
    "question = \"What should I use when I wanna split text involving regular expressions?\"\n",
    "\n",
    "output = llm(context + question)\n",
    "\n",
    "# Did strip the text to remove the leading and trailing whitespace\n",
    "print (output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it would be really trouble when it is required to even select the correct data into context. This is called \"[document retrieval](https://python.langchain.com/en/latest/modules/indexes/retrievers.html)\" and tightly related to AI Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using embeddings\n",
    "\n",
    "Basically means using embedding to find answers through a longer given text, that is, splitting text, embedding the chunks, putting them in a Db for querying. [Here is a tutorial for doing a book.](https://www.youtube.com/watch?v=h0DHDp1FbmQ).\n",
    "\n",
    "A key point is selecting relevant chunks by **pulling similar texts based on comparisons of vector embeddings**.\n",
    "\n",
    "*Now I am going to use self-hosted models.*\n",
    "\n",
    "**update: So far serious performance and accuracy issues with local model, more study work required to be done. But 0 issue with smooth experience using OpenAI API, strange, performance difference should not be this big**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-chat.bin\n",
      "mpt_model_load: loading model from '/Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-chat.bin' - please wait ...\n",
      "mpt_model_load: n_vocab        = 50432\n",
      "mpt_model_load: n_ctx          = 2048\n",
      "mpt_model_load: n_embd         = 4096\n",
      "mpt_model_load: n_head         = 32\n",
      "mpt_model_load: n_layer        = 32\n",
      "mpt_model_load: alibi_bias_max = 8.000000\n",
      "mpt_model_load: clip_qkv       = 0.000000\n",
      "mpt_model_load: ftype          = 2\n",
      "mpt_model_load: ggml ctx size = 5653.09 MB\n",
      "mpt_model_load: kv self size  = 1024.00 MB\n",
      "mpt_model_load: ........................ done\n",
      "mpt_model_load: model size =  4629.02 MB / num tensors = 194\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "# The vectorstore we'll be using\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# The LangChain component we'll use to get the documents\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# The easy document loader for text\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The embedding engine that will convert our text to vectors, doc: https://api.python.langchain.com/en/latest/modules/embeddings.html#langchain.embeddings.HuggingFaceEmbeddings.embed_documents\\\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Locaing text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For loading local models\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "model_path=\"/Users/hann/Projects/RES_LLM/private-llm/models/\"\n",
    "#  models are ggml-mpt-7b-chat.bin, ggml-gpt4all-l13b-snoozy.bin, ggml-gpt4all-j-v1.3-groovy.bin, ggml-mpt-7b-instruct.bin, ggml-v3-13b-hermes-q5_1.bin\n",
    "models={\n",
    "    \"groovy\": os.path.join(model_path, \"ggml-gpt4all-j-v1.3-groovy.bin\"),\n",
    "    \"snoozy\": os.path.join(model_path, \"ggml-gpt4all-l13b-snoozy.bin\"),\n",
    "    \"7bchat\": os.path.join(model_path, \"ggml-mpt-7b-chat.bin\"),\n",
    "    \"7binstruct\": os.path.join(model_path, \"ggml-mpt-7b-instruct.bin\"),\n",
    "    \"hermes\": os.path.join(model_path, \"ggml-v3-13b-hermes-q5_1.bin\")\n",
    "}\n",
    "\n",
    "#  \"groovy\", \"snoozy\",  \"7bchat\", \"7binstruct\", \"hermes\"\n",
    "md = \"7bchat\"\n",
    "llm = GPT4All(model=models[md], callbacks=[StreamingStdOutCallbackHandler()])\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 16523 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Document loading\n",
    "loader = TextLoader('theheartofabrokenstory.txt')\n",
    "doc = loader.load()\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 12 documents that have an average of 1,599 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding engine\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# print(openai_api_key)\n",
    "\n",
    "# embeddings_openai = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the retrieval engine and ask questions. The retriever will get similar documents and combine the question with the context. Then the LLM will answer the question.\n",
    "\n",
    "**The original chain has a bit problem (or outdated), now here is the workable version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=docsearch.as_retriever())\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "qa = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The man described in these extracts is an office worker.  Justin's job title appears as \" head printer\" and we can infer from this that his profession or career must have been something related\n",
      " The man described in these extracts is an office worker.  Justin's job title appears as \" head printer\" and we can infer from this that his profession or career must have been something related"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The man described in these extracts is an office worker.  Justin\\'s job title appears as \" head printer\" and we can infer from this that his profession or career must have been something related'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"What does the author describe as good work?\"\n",
    "qa.run(\"What was the occupation of Justin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If you wanted to do more you would hook this up to a cloud vector database, use a tool like metal and start managing your documents, with external data sources\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "[Official documentation about this.](https://python.langchain.com/en/latest/use_cases/extraction.html)\n",
    "\n",
    "Extraction is the process of parsing data from a piece of text. This is commonly used with output parsing in order to structure our data.\n",
    "\n",
    "E.g., extract a structured row from a sentence to insert into a database, extract multiple rows from a long document to insert into a database, extracting parameters from a user query to make an API call. \n",
    "\n",
    "A popular library for extraction is [Kor](https://eyurtsev.github.io/kor/), check for advanced usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To help construct our Chat Messages\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# To parse outputs and get structured data back\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "\n",
    "# For loading local models\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "model_path=\"/Users/hann/Projects/RES_LLM/private-llm/models/\"\n",
    "#  models are ggml-mpt-7b-chat.bin, ggml-gpt4all-l13b-snoozy.bin, ggml-gpt4all-j-v1.3-groovy.bin, ggml-mpt-7b-instruct.bin, ggml-v3-13b-hermes-q5_1.bin\n",
    "models={\n",
    "    \"groovy\": os.path.join(model_path, \"ggml-gpt4all-j-v1.3-groovy.bin\"),\n",
    "    \"snoozy\": os.path.join(model_path, \"ggml-gpt4all-l13b-snoozy.bin\"),\n",
    "    \"7bchat\": os.path.join(model_path, \"ggml-mpt-7b-chat.bin\"),\n",
    "    \"7binstruct\": os.path.join(model_path, \"ggml-mpt-7b-instruct.bin\"),\n",
    "    \"hermes\": os.path.join(model_path, \"ggml-v3-13b-hermes-q5_1.bin\")\n",
    "}\n",
    "#  \"groovy\", \"snoozy\",  \"7bchat\", \"7binstruct\", \"hermes\"\n",
    "md = \"groovy\"\n",
    "\n",
    "# 1 for local, 0 for huggingface\n",
    "use_local = 1 \n",
    "\n",
    "# comment this line for using local model\n",
    "use_local = 0\n",
    "\n",
    "if use_local==0:\n",
    "    chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n",
    "else:\n",
    "    chat_model = GPT4All(model=models[md])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Vanilla extraction\n",
    "\n",
    "Simple example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"{'Apple': '🍎', 'Pear': '🍐', 'kiwi': '🥝'}\" additional_kwargs={} example=False\n",
      "<class 'langchain.schema.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
    "Return the fruit name and emojis in a python dictionary\n",
    "\"\"\"\n",
    "\n",
    "fruit_names = \"\"\"\n",
    "Apple, Pear, this is an kiwi\n",
    "\"\"\"\n",
    "\n",
    "# Make your prompt which combines the instructions w/ the fruit names\n",
    "prompt = str(instructions + fruit_names)\n",
    "\n",
    "# Call the LLM\n",
    "output = chat_model([HumanMessage(content=prompt)])\n",
    "# output = chat_model(prompt)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn this into a proper python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': '🍎', 'Pear': '🍐', 'kiwi': '🥝'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "output_dict = eval(output.content)\n",
    "\n",
    "print (output_dict)\n",
    "print (type(output_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LangChain's Response Schema\n",
    "LangChain's response schema will does two things for us:\n",
    "\n",
    "Autogenerate a prompt with bonafide format instructions. This is great because I don't need to worry about the prompt engineering side, I'll leave that up to LangChain!\n",
    "\n",
    "Read the output from the LLM and turn it into a proper python object for me\n",
    "\n",
    "Here I define the schema I want. I'm going to pull out the song and artist that a user wants to play from a pseudo chat message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# The schema I want it to output\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
    "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
    "]\n",
    "\n",
    "# The parser that will look for the LLM output in my schema and return it back to me\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# The format instructions that LangChain makes. Let's look at them\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a command from the user, extract the artist and song names \n",
      "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"artist\": string  // The name of the musical artist\n",
      "\t\"song\": string  // The name of the song that the artist plays\n",
      "}\n",
      "```\n",
      "I really like So Young by Portugal. The Man\n"
     ]
    }
   ],
   "source": [
    "# The prompt template that brings it all together\n",
    "# Note: This is a different prompt template than before because we are using a Chat Model\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
    "                                                    {format_instructions}\\n{user_prompt}\")  \n",
    "    ],\n",
    "    input_variables=[\"user_prompt\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
    "print (fruit_query.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "fruit_output = chat_model(fruit_query.to_messages())\n",
    "output = output_parser.parse(fruit_output.content)\n",
    "\n",
    "print (output)\n",
    "print (type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "[Official documentation about this.](https://python.langchain.com/en/latest/use_cases/evaluation.html)\n",
    "\n",
    "Evaluation is the process of doing quality checks on the output. Normally, deterministic code has tests we can run, but judging the output of LLMs is more difficult because of the unpredictableness and variability of natural language. LangChain provides tools that aid us in this journey.\n",
    "\n",
    "E.g., Run quality checks on your summarization or Question & Answer pipelines, check the output of you summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-instruct.bin\n",
      "mpt_model_load: loading model from '/Users/hann/Projects/RES_LLM/private-llm/models/ggml-mpt-7b-instruct.bin' - please wait ...\n",
      "mpt_model_load: n_vocab        = 50432\n",
      "mpt_model_load: n_ctx          = 2048\n",
      "mpt_model_load: n_embd         = 4096\n",
      "mpt_model_load: n_head         = 32\n",
      "mpt_model_load: n_layer        = 32\n",
      "mpt_model_load: alibi_bias_max = 8.000000\n",
      "mpt_model_load: clip_qkv       = 0.000000\n",
      "mpt_model_load: ftype          = 2\n",
      "mpt_model_load: ggml ctx size = 5653.09 MB\n",
      "mpt_model_load: kv self size  = 1024.00 MB\n",
      "mpt_model_load: ........................ done\n",
      "mpt_model_load: model size =  4629.02 MB / num tensors = 194\n"
     ]
    }
   ],
   "source": [
    "# Embeddings, store, and retrieval\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Model and doc loader\n",
    "# from langchain import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Eval!\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "# For loading local models\n",
    "from langchain.llms import GPT4All\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "model_path=\"/Users/hann/Projects/RES_LLM/private-llm/models/\"\n",
    "#  models are ggml-mpt-7b-chat.bin, ggml-gpt4all-l13b-snoozy.bin, ggml-gpt4all-j-v1.3-groovy.bin, ggml-mpt-7b-instruct.bin, ggml-v3-13b-hermes-q5_1.bin\n",
    "models={\n",
    "    \"groovy\": os.path.join(model_path, \"ggml-gpt4all-j-v1.3-groovy.bin\"),\n",
    "    \"snoozy\": os.path.join(model_path, \"ggml-gpt4all-l13b-snoozy.bin\"),\n",
    "    \"7bchat\": os.path.join(model_path, \"ggml-mpt-7b-chat.bin\"),\n",
    "    \"7binstruct\": os.path.join(model_path, \"ggml-mpt-7b-instruct.bin\"),\n",
    "    \"hermes\": os.path.join(model_path, \"ggml-v3-13b-hermes-q5_1.bin\")\n",
    "}\n",
    "#  \"groovy\", \"snoozy\",  \"7bchat\", \"7binstruct\", \"hermes\"\n",
    "md = \"7binstruct\"\n",
    "\n",
    "# 1 for local, 0 for huggingface\n",
    "use_local = 1 \n",
    "\n",
    "# comment this line for using local model\n",
    "# use_local = 0\n",
    "\n",
    "if use_local==0:\n",
    "    llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "else:\n",
    "    llm = GPT4All(model=models[md])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document\n",
      "You have 16523 characters in that document\n"
     ]
    }
   ],
   "source": [
    "# Our long essay from before\n",
    "loader = TextLoader('theheartofabrokenstory.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "print (f\"You have {len(doc)} document\")\n",
    "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 8 documents that have an average of 2,310 characters (smaller pieces)\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "\n",
    "# Get the total number of characters so we can see the average later\n",
    "num_total_characters = sum([len(x.page_content) for x in docs])\n",
    "\n",
    "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding engine\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Embeddings and docstore\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your retrieval chain. Notice how I have an input_key parameter now. This tells the chain which key from a dictionary I supply has my prompt/query in it. I specify question to match the question in the dict below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=docsearch.as_retriever(), input_key=\"question\")\n",
    "\n",
    "# qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "# chain = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever(),input_key=\"question\")\n",
    "# /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll pass a list of questions and ground truth answers to the LLM that I know are correct (I validated them as a human)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answers = [\n",
    "    {'question' : \"What is the occupation of Justin?\", 'answer' : 'printer’s assistant'},\n",
    "    {'question' : \"In which city the story has happened\", 'answer' : 'New York'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposed to be like:\n",
    "\n",
    "```json\n",
    "[{'question': 'What is the occupation of Justin?',\n",
    "  'answer': 'printer’s assistant',\n",
    "  'result': ' Jsutin is a printer’s assistant.'},\n",
    " {'question': 'In which city the story has happened?',\n",
    "  'answer': 'New York',\n",
    "  'result': ' The story has happened in New York.'},]\n",
    "```\n",
    "\n",
    "I'll use chain.apply to run both my questions one by one separately.\n",
    "\n",
    "One of the cool parts is that I'll get my list of question and answers dictionaries back, but there'll be another key in the dictionary result which will be the output from the LLM.\n",
    "\n",
    "Note: I specifically made my 2nd question ambigious and tough to answer in one pass so the LLM would get it incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"I suppose I'm just one Of The Millions Who Was Never Meanth To Give Orders.\"\n",
      " \"I suppose me as having been one Of The Millions Who Was Never Meanth To Give Orders.\"\n",
      " \"I suppose I'm just one Of The Millions Who Was Never Meanth To Give Orders.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPT: reached the end of the context window so resizing\n",
      "Exception ignored on calling ctypes callback function: <function LLModel._recalculate_callback at 0x113c49e50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hann/miniconda3/envs/llm/lib/python3.8/site-packages/gpt4all/pyllmodel.py\", line 334, in _recalculate_callback\n",
      "    @staticmethod\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "predictions = chain.apply(question_answers)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have the LLM compare my ground truth answer (the answer key) with the result from the LLM (result key).\n",
    "\n",
    "Or simply, we are asking the LLM to grade itself. What a wild world we live in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INCORRECT\n",
      " INCORRECT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': ' INCORRECT'}, {'text': ' INCORRECT'}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start your eval chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
    "graded_outputs = eval_chain.evaluate(question_answers,\n",
    "                                     predictions,\n",
    "                                     question_key=\"question\",\n",
    "                                     prediction_key=\"result\",\n",
    "                                     answer_key='answer')\n",
    "graded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
